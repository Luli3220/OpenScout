{
  "username": "peggyrayzis",
  "agent_b_context": "\n=== PR #2594 in modelcontextprotocol/servers (Stars: 74551) ===\n\n=== PR #2377 in apollographql/apollo-client (Stars: 19683) ===\n\n=== PR #1019 in apollographql/apollo-server (Stars: 13932) ===\n\n=== Fallback Source Audit: backend/src/services/llm.service.ts (Size: 6546 bytes) ===\nimport { openai, openAIConfig, isOpenAIConfigured } from '../config/openai.config';\nimport { VectorSearchResult } from '../models/document.model';\n\nexport interface ChatMessage {\n  role: 'system' | 'user' | 'assistant';\n  content: string;\n}\n\nexport interface TokenUsage {\n  promptTokens: number;\n  completionTokens: number;\n  totalTokens: number;\n}\n\nexport interface ChatCompletionResult {\n  content: string;\n  usage: TokenUsage;\n}\n\nexport interface ChatCompletionOptions {\n  temperature?: number;\n  maxTokens?: number;\n  systemPrompt?: string;\n}\n\nexport class LLMService {\n  /**\n   * Generate a chat completion with optional RAG context\n   */\n  async generateChatCompletion(\n    userMessage: string,\n    context?: VectorSearchResult[],\n    conversationHistory?: ChatMessage[],\n    options?: ChatCompletionOptions\n  ): Promise<ChatCompletionResult> {\n    if (!isOpenAIConfigured()) {\n      throw new Error('OpenAI API key is not configured. Please set OPENAI_API_KEY in your .env file.');\n    }\n\n    const messages: ChatMessage[] = [];\n\n    // Add system prompt\n    const systemPrompt = options?.systemPrompt || this.buildSystemPrompt(context);\n    messages.push({\n      role: 'system',\n      content: systemPrompt,\n    });\n\n    // Add conversation history if provided\n    if (conversationHistory && conversationHistory.length > 0) {\n      messages.push(...conversationHistory);\n    }\n\n    // Add current user message\n    messages.push({\n      role: 'user',\n      content: userMessage,\n    });\n\n    try {\n      const response = await openai.chat.completions.create({\n        model: openAIConfig.chat.model,\n        messages: messages,\n        temperature: options?.temperature ?? openAIConfig.chat.temperature,\n        max_tokens: options?.maxTokens ?? openAIConfig.chat.maxTokens,\n      });\n\n      if (!response.choices || response.choices.length === 0) {\n        throw new Error('No response generated from OpenAI');\n      }\n\n      const content = response.choices[0].message.content || '';\n      const usage: TokenUsage = {\n        promptTokens: response.usage?.prompt_tokens || 0,\n        completionTokens: response.usage?.completion_tokens || 0,\n        totalTokens: response.usage?.total_tokens || 0,\n      };\n\n      return { content, usage };\n    } catch (error: any) {\n      console.error('Error generating chat completion:', error);\n\n      if (error.status === 401) {\n        throw new Error('Invalid OpenAI API key');\n      } else if (error.status === 429) {\n        throw new Error('OpenAI rate limit exceeded. Please try again later.');\n      } else if (error.status === 500) {\n        throw new Error('OpenAI service error. Please try again later.');\n      }\n\n      throw new Error(`Failed to generate response: ${error.message}`);\n    }\n  }\n\n  /**\n   * Build system prompt with RAG context\n   */\n  private buildSystemPrompt(context?: VectorSearchResult[]): string {\n    let systemPrompt = `You are a helpful AI customer service assistant. You provide accurate, relevant, and concise ans\n... (file truncated)\n"
}