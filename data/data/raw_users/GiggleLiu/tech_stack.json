[
  {
    "name": "GiggleLiu/NiLang.jl",
    "stars": 254,
    "description": "A differential eDSL that can run faster than light and go back to the past.",
    "languages_breakdown": {
      "Julia": 397533,
      "Makefile": 798
    },
    "files": {
      ".github/workflows/ci.yml": "name: CI\non:\n  - push\n  - pull_request\njobs:\n  test:\n    name: Julia ${{ matrix.version }} - ${{ matrix.os }} - ${{ matrix.arch }} - ${{ github.event_name }}\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        version:\n          - '1.5'\n          - 'nightly'\n        os:\n          - ubuntu-latest\n          - macOS-latest\n          - windows-latest\n        arch:\n          - x64\n    steps:\n      - uses: actions/checkout@v2\n      - uses: julia-actions/setup-julia@v1\n        with:\n          version: ${{ matrix.version }}\n          arch: ${{ matrix.arch }}\n      - uses: actions/cache@v1\n        env:\n          cache-name: cache-artifacts\n        with:\n          path: ~/.julia/artifacts\n          key: ${{ runner.os }}-test-${{ env.cache-name }}-${{ hashFiles('**/Project.toml') }}\n          restore-keys: |\n            ${{ runner.os }}-test-${{ env.cache-name }}-\n            ${{ runner.os }}-test-\n            ${{ runner.os }}-\n      - uses: julia-actions/julia-buildpkg@v1\n      - uses: julia-actions/julia-runtest@v1\n      - uses: julia-actions/julia-processcoverage@v1\n      - uses: codecov/codecov-action@v1\n        with:\n          file: lcov.info\n  docs:\n    name: Documentation\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - uses: julia-actions/setup-julia@v1\n        with:\n          version: '1'\n      - run: |\n          julia --project=docs -e '\n            using Pkg\n            Pkg.develop(PackageSpec(path=pwd()))\n            Pkg.instantiate()'\n      - run: |\n          julia --project=docs -e '\n            using Documenter: doctest\n            using NiLang\n            doctest(NiLang)'\n      - run: julia --project=docs docs/make.jl\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          DOCUMENTER_KEY: ${{ secrets.DOCUMENTER_KEY }}\n",
      "README.md": "<img src=\"docs/src/asset/logo3.png\" width=500px/>\n\nNiLang.jl (逆lang), is a reversible domain-specific language (DSL) that allow a program to go back to the past.\n\n* Requires Julia version >= 1.3,\n\nNiLang features:\n\n* any program written in NiLang is differentiable,\n* a reversible language with abstraction and arrays,\n* complex values\n* reversible logarithmic number system\n\n![CI](https://github.com/GiggleLiu/NiLang.jl/workflows/CI/badge.svg)\n[![codecov](https://codecov.io/gh/GiggleLiu/NiLang.jl/branch/master/graph/badge.svg?token=th86D4USSX)](https://codecov.io/gh/GiggleLiu/NiLang.jl)\n\nThe main docs can be found here:\n[![](https://img.shields.io/badge/docs-stable-blue.svg)](https://giggleliu.github.io/NiLang.jl/stable/)\n[![](https://img.shields.io/badge/docs-dev-blue.svg)](https://giggleliu.github.io/NiLang.jl/dev/)\n\nThere are also some Pluto-based notebooks:\n* [tutorial](https://giggleliu.github.io/NiLang.jl/dev/notebooks/basic.html)\n* [documentation](https://giggleliu.github.io/NiLang.jl/dev/notebooks/documentation.html)\n* [Billiard ball model cellular automata](https://giggleliu.github.io/NiLang.jl/dev/notebooks/margolus.html)\n\n> The strangeness of reversible computing is mainly due to\n> our lack of experience with it.—Henry Baker, 1992\n\n## To Start\n```\npkg> add NiLang\n```\n\n## An example: Compute the norm of a vector\n```julia\njulia> using NiLang\n\njulia> @i function f(res, y, x)\n           for i=1:length(x)\n               y += x[i] ^ 2\n           end\n           res += sqrt(y)\n       end\n\njulia> res_out, y_out, x_out = f(0.0, 0.0, [1, 2, 3.0])\n(3.7416573867739413, 14.0, [1.0, 2.0, 3.0])\n\njulia> (~f)(res_out, y_out, x_out)  # automatically generated inverse program.\n(0.0, 0.0, [1.0, 2.0, 3.0])\n        \njulia> ∂res, ∂y, ∂x = NiLang.AD.gradient(Val(1), f, (0.0, 0.0, [1, 2, 3.0])) \n    # automatic differentiation, `Val(1)` means the first argument of `f` is the loss.\n(1.0, 0.1336306209562122, [0.2672612419124244, 0.5345224838248488, 0.8017837257372732])\n```\n\nThe performance of reversible programming automatic differentiation is much better than most traditional frameworks. Here is why, and how it works,\n\n![how it works](docs/src/asset/adprog.png)\n\n## Check our [paper](https://arxiv.org/abs/2003.04617)\n\n```bibtex\n@misc{Liu2020,\n    title={Differentiate Everything with a Reversible Programming Language},\n    author={Jin-Guo Liu and Taine Zhao},\n    year={2020},\n    eprint={2003.04617},\n    archivePrefix={arXiv},\n    primaryClass={cs.PL}\n}\n```\n"
    }
  },
  {
    "name": "GiggleLiu/marburg",
    "stars": 103,
    "description": "physics meets neural networks",
    "languages_breakdown": {
      "Jupyter Notebook": 1201049,
      "C++": 1533,
      "Cuda": 1158,
      "Makefile": 283
    },
    "files": {
      "README.md": "# Deep Learning and Quantum Many-Body Physics - Hands on Session\n\n## Table of Contents\nWe have prepaired four examples\n\n* Computation Graphs and Back Propagation ([online](https://goo.gl/6d2sei), [solution](https://goo.gl/DZtidF))\n* Normalization flow for sampling ([online](https://goo.gl/8Caymh), [solution](https://goo.gl/FhAHRZ))\n* Restricted Boltzmann Machine for image restoration ([online](https://goo.gl/d7kPzy), [solution](https://goo.gl/VxYYQX))\n* Deep Neural Network as a Quantum Wave Function Ansatz ([online](https://goo.gl/vPFtdU))\n\nThey have been uploaded to both Google drive and notebooks folder in this repository. Have fun! \n\n## Preparations\nYou may use **either** local or online accesses to our python notebooks.\n\nIf you are not using an Nvidia GPU or its driver are not properly configured, online access is recommended,\notherwise you may loss some fun in this session.\n\n### Local\nRequires a Linux or OSX operating system (required by pytorch).\n\nSet up your python environment\n\n* python 3.6\n* install python libraries **numpy**, **matplotlib** and **ipython**\n\n```bash\npip install numpy matplotlib ipython\n```\n\n* install PyTorch following [installation guide](http://pytorch.org/).\n    * select CUDA 8 if you have an Nvidia GPU with properly configured driver\n    * select CUDA None if your don't\n\nClone this repository https://github.com/GiggleLiu/marburg.git to your local host.\nChange directory to project home, check your installation by typing `ipython notebook env_test.ipynb` and run this test script.\n\n### Online\nOnline means you can try CUDA programming without having an local NVIDIA GPU.\n\n1. Sign up and sign in [Google drive](https://drive.google.com/)\n2. Connect Google drive with [Google Colaboratory](https://colab.research.google.com)\n    - right click on google drive page\n    - More\n    - Connect more apps\n    - search \"Colaboratory\" and \"CONNECT\"\n3. Open the following online notebook link\n    https://drive.google.com/file/d/1MLcG21zqSU9AvbY4siD4NqqbB2uwP2p2/view?usp=sharing\n4. Setup GPU following instructions in above notebook\n5. You can make a copy of notebook to your google drive (File Menu) to save your edits.\n\n## Documentations\n\n* lecture notes: *docs/LectureNoteonML.pdf*\n* hands on slides: *docs/ML-handson.pdf*\n\n## Authors\n\n* Jin-Guo Liu <cacate0129@iphy.ac.cn>\n* Shuo-Hui Li <contact_lish@iphy.ac.cn>\n* Lei Wang <wanglei@iphy.ac.cn>\n"
    }
  },
  {
    "name": "GiggleLiu/viznet",
    "stars": 55,
    "description": "network visualization toolkit",
    "languages_breakdown": {
      "Python": 127443,
      "Jupyter Notebook": 7082
    },
    "files": {
      "requirements.txt": "numpy\nscipy\nmatplotlib\n",
      "README.md": "# viznet - a network visualization toolbox\n![](docs/images/viznetlogo.jpg)\nviznet is designed for visualizing networks composed of nodes and edges, e.g. tensor networks, neural networks and quantum circuits. \n\nIt is based on and compatible with matplotlib. The theme brush (for both node and edge) makes the design itself interesting, getting you free from fine tuning the node and wire parameters for hours.\n\n## To Install\n```bash\n    $ pip install viznet\n```\n\nor for the latest version\n```bash\n    $ git clone https://github.com/GiggleLiu/viznet.git\n    $ cd viznet\n    $ pip install -r requirements.txt\n    $ python setup.py install\n```\n\n## To Run Examples\n```bash\n    $ cd viznet\n    $ python apps/nn/bm.py      # example on neural network\n    $ python apps/tn/tebd.py    # example on tensor network\n    $ python apps/qc/ghz.py     # example on quantum circuit\n```\nyou will get something like\n\nBoltzmann Machine       | TEBD                      \n----------------------- | -------------------------\n![](docs/images/bm.png) | ![](docs/images/tebd.png)\n\n Quantum Circuit           |  Graph Theory\n --------------------------| -------------------------------\n ![](docs/images/ghz4.png) | ![](docs/images/contract.gif)\n\nThe theme for neural network follows from [Neural Network Zoo Page](http://www.asimovinstitute.org/neural-network-zoo/),\n\nThe theme for quantum circuits follows from [ProjectQ](https://github.com/ProjectQ-Framework/ProjectQ.git).\n\n## Author\n\nThe first release of viznet (v0.1) was developed by [Jin-Guo Liu](https://giggleliu.github.io/)  in the group of Lei Wang at IOP China.\n\n## Documentation\n* Go through notebook `docs/viznet_basic.ipynb` ([online](https://drive.google.com/file/d/1mP9DOoTR4JEhd-ILVXfggpBJOyGatiws/view?usp=sharing)) as a quick introduction\n* Click [here](http://viznet.readthedocs.io/en/latest/) to read the docs!\n"
    }
  }
]