[
  {
    "name": "rentainhe/visualization",
    "stars": 447,
    "description": "a collection of visualization function",
    "languages_breakdown": {
      "Python": 18731
    },
    "files": {
      "README.md": "# visualization\na collection of visualization operation for easier usage, check [usage](#usage) for a quick start.\n\n## New Features\n**2021/10/4**\n- Add `draw_line_chart` function, please check [drawer.py](/visualize/line_chart/drawer.py)\n\n**2021/09/29**\n- Add pip installation\n- Build a cleaner repo\n\n## Contents\n### Visualization Function\n- [__Grid Attention Visualization__](/visualize/grid_attention_visualization/)\n- [__Region Attention Visualization__](/visualize/region_attention_visualization/)\n- [__Draw Line Chart__](/visualize/line_chart/drawer.py)\n\n### Learning Notes Sharing\n- [__Image Reading Difference__](/notes)\n\n### Relative Blogs\n- [Region Attention Map Visualize](https://zhuanlan.zhihu.com/p/364486740)\n- [Grid Attention Map Visualize](https://zhuanlan.zhihu.com/p/356798637)\n\n## Installation\n```bash\npip install visualize==0.5.1\n```\n\n## Usage\n<details>\n<summary> <b> Run Example </b> </summary>\n\nYou can try [example.py](/example.py) by cloning this repo for a quick start.\n```bash\ngit clone https://github.com/rentainhe/visualization.git\npython example.py\n```\nresults will be saved to `./test_grid_attention` and `./test_region_attention`\n</details>\n\n<details>\n<summary> <b> Region Attention Visualization </b> </summary>\n\n**download the [example.jpg](/visualize/test_data/example.jpg) to any folder you like**\n```bash\n$ wget https://github.com/rentainhe/visualization/blob/master/visualize/test_data/example.jpg\n```\n**build the following python script for a quick start**\n```python\nimport numpy as np\nfrom visualize import visualize_region_attention\n\nimg_path=\"path/to/example.jpg\"\nsave_path=\"example\"\nattention_retio=1.0\nboxes = np.array([[14, 25, 100, 200], [56, 75, 245, 300]], dtype='int')\nboxes_attention = [0.36, 0.64]\nvisualize_region_attention(img_path,\n                           save_path=save_path, \n                           boxes=boxes, \n                           box_attentions=boxes_attention, \n                           attention_ratio=attention_retio,\n                           save_image=True,\n                           save_origin_image=True,\n                           quality=100)\n```\n- `img_path`: where to load the original image\n- `boxes`: a list of coordinates for the bounding boxes\n- `box_attentions`: a list of attention scores for each bounding box\n- `attention_ratio`: a special param, if you set the attention_ratio larger, it will make the attention map look more shallow. Just try!\n- `save_image=True`: save the image with attention map or not, e.g., default: True.\n- `save_original_image=True`: save the original image at the same time, e.g., default: True\n\n**Note that you can check [Region Attention Visualization](/visualize/region_attention_visualization/) here for more details**\n\n</details>\n\n<details>\n<summary> <b> Grid Attention Visualization</b> </summary>\n\n**download the [example.jpg](/visualize/test_data/example.jpg) to any folder you like**\n```bash\n$ wget https://github.com/rentainhe/visualization/blob/master/visualize/tes\n... (truncated)"
    }
  },
  {
    "name": "rentainhe/pytorch-distributed-training",
    "stars": 285,
    "description": "Simple tutorials on Pytorch DDP training",
    "languages_breakdown": {
      "Python": 43842
    },
    "files": {
      "README.md": "## pytorch-distributed-training\nDistribute Dataparallel (DDP) Training on Pytorch\n\n### Features\n* Easy to study DDP training\n* You can directly copy this code for a quick start\n* Learning Notes Sharing(with `√`means finished):\n  - [x] [Basic Theory](https://github.com/rentainhe/pytorch-distributed-training/blob/master/tutorials/0.%20Basic%20Theory.md)\n  - [x] [Pytorch Gradient Accumulation](https://github.com/rentainhe/pytorch-distributed-training/blob/master/tutorials/1.%20Gradient%20Accumulation.md)\n  - [x] [More Details of DDP Training](https://github.com/rentainhe/pytorch-distributed-training/blob/master/tutorials/2.%20DDP%20Training%20Details.md)\n  - [x] [DDP training with apex](https://github.com/rentainhe/pytorch-distributed-training/blob/master/tutorials/4.%20DDP%20with%20apex.md)\n  - [x] [Accelerate-on-Accelerate DDP Training Tricks](https://github.com/rentainhe/pytorch-distributed-training/blob/master/tutorials/3.%20DDP%20Training%20Tricks.md)\n  - [x] [DP and DDP 源码解读](https://github.com/rentainhe/pytorch-distributed-training/blob/master/tutorials/5.%20DP%20and%20DDP.md)\n\n### Good Notes\n分享一些网上优质的笔记\n- [分布式训练（理论篇）](https://zhuanlan.zhihu.com/p/129912419)\n- [当代研究生应当掌握的并行训练方法（单机多卡）](https://zhuanlan.zhihu.com/p/98535650)\n\n### TODO\n- [ ] 完成DP和DDP源码解读笔记(当前进度50%)\n- [ ] 修改代码细节, 复现实验结果\n\n### Quick start\n想直接运行查看结果的可以执行以下命令, 注意一定要用`--ip`和`--port`来指定主机的`ip`地址以及空闲的`端口`，否则可能无法运行\n- [dataparaller.py](https://github.com/rentainhe/pytorch-distributed-training/blob/master/dataparallel.py)\n```bash\n$ python dataparallel.py --gpu 0,1,2,3\n```\n\n- [distributed.py](https://github.com/rentainhe/pytorch-distributed-training/blob/master/distributed.py)\n```bash\n$ CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 distributed.py\n```\n\n- [distributed_mp.py](https://github.com/rentainhe/pytorch-distributed-training/blob/master/distributed_mp.py)\n```bash\n$ CUDA_VISIBLE_DEVICES=0,1,2,3 python distributed_mp.py\n```\n\n- [distributed_apex.py](https://github.com/rentainhe/pytorch-distributed-training/blob/master/distributed_apex.py)\n```bash\n$ CUDA_VISIBLE_DEVICES=0,1,2,3 python distributed_apex.py\n```\n\n- `--ip=str`, e.g `--ip='10.24.82.10'` 来指定主进程的ip地址\n- `--port=int`, e.g `--port=23456` 来指定启动端口号\n- `--batch_size=int`, e.g `--batch_size=128` 设定训练batch_size\n\n- [distributed_gradient_accumulation.py](https://github.com/rentainhe/pytorch-distributed-training/blob/master/distributed_gradient_accumulation.py)\n```bash\n$ CUDA_VISIBLE_DEVICES=0,1,2,3 python distributed_apex.py\n```\n- `--ip=str`, e.g `--ip='10.24.82.10'` 来指定主进程的ip地址\n- `--port=int`, e.g `--port=23456` 来指定启动端口号\n- `--grad_accu_steps=int`, e.g `--grad_accu_steps=4'` 来指定gradient_step\n\n\n### Comparison\n结果不够准确，GPU状态不同结果可能差异较大\n\n默认情况下都使用`SyncBatchNorm`, 这会导致执行速度变慢一些，因为需要增加进程之间的通讯来计算`BatchNorm`, 但有利于保证准确率\n\nConcepts\n- [apex](https://github.com/NVIDIA/apex)\n- DP: `DataParallel`\n- DDP: `DistributedDataParallel`\n\nEnvironments\n- 4 × 2080Ti\n\n|model|dataset|training method|time(seconds/epoch)|Top-1 accuracy\n|:\n... (truncated)"
    }
  },
  {
    "name": "rentainhe/TRAR-VQA",
    "stars": 68,
    "description": "[ICCV 2021] Official implementation of the paper \"TRAR: Routing the Attention Spans in Transformers for Visual Question Answering\"",
    "languages_breakdown": {
      "Python": 116416
    },
    "files": {
      "requirements.txt": "spacy >= 2.0.18\nnumpy >= 1.16.2\n\n\n",
      "README.md": "# TRAnsformer Routing Networks (TRAR)\r\nThis is an official implementation for ICCV 2021 paper [\"TRAR: Routing the Attention Spans in Transformers for Visual Question Answering\"](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_TRAR_Routing_the_Attention_Spans_in_Transformer_for_Visual_Question_ICCV_2021_paper.pdf). It currently includes the code for training TRAR on **VQA2.0** and **CLEVR** dataset. Our TRAR model for REC task is coming soon.\r\n\r\n## Updates\r\n- (2021/10/10) Release our TRAR-VQA project.\r\n- (2021/08/31) Release our pretrained `CLEVR` TRAR model on `train` split: [TRAR CLEVR Pretrained Models](MODEL.md#CLEVR).\r\n- (2021/08/18) Release our pretrained TRAR model on `train+val` split and `train+val+vg` split: [VQA-v2 TRAR Pretrained Models](MODEL.md#VQA-v2) \r\n- (2021/08/16) Release our `train2014`, `val2014` and `test2015` data. Please check our dataset setup page [DATA.md](DATA.md) for more details.\r\n- (2021/08/15) Release our pretrained weight on `train` split. Please check our model page [MODEL.md](MODEL.md) for more details.\r\n- (2021/08/13) The project page for TRAR is avaliable.\r\n\r\n## Introduction\r\n**TRAR vs Standard Transformer**\r\n<p align=\"center\">\r\n\t<img src=\"misc/trar_block.png\" width=\"550\">\r\n</p>\r\n\r\n**TRAR Overall**\r\n<p align=\"center\">\r\n\t<img src=\"misc/trar_overall.png\" width=\"550\">\r\n</p>\r\n\r\n## Table of Contents\r\n0. [Installation](#Installation)\r\n1. [Dataset setup](#Dataset-setup)\r\n2. [Config Introduction](#Config-Introduction)\r\n3. [Training](#Training)\r\n4. [Validation and Testing](#Validation-and-Testing)\r\n5. [Models](#Models)\r\n\r\n### Installation\r\n- Clone this repo\r\n```bash\r\ngit clone https://github.com/rentainhe/TRAR-VQA.git\r\ncd TRAR-VQA\r\n```\r\n\r\n- Create a conda virtual environment and activate it\r\n```bash\r\nconda create -n trar python=3.7 -y\r\nconda activate trar\r\n```\r\n\r\n- Install `CUDA==10.1` with `cudnn7` following the [official installation instructions](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html)\r\n- Install `Pytorch==1.7.1` and `torchvision==0.8.2` with `CUDA==10.1`:\r\n```bash\r\nconda install pytorch==1.7.1 torchvision==0.8.2 cudatoolkit=10.1 -c pytorch\r\n```\r\n- Install [Spacy](https://spacy.io/) and initialize the [GloVe](https://github-releases.githubusercontent.com/84940268/9f4d5680-4fed-11e9-9dd2-988cce16be55?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210815%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210815T072922Z&X-Amz-Expires=300&X-Amz-Signature=1bd1bd4fc52057d8ac9eec7720e3dd333e63c234abead471c2df720fb8f04597&X-Amz-SignedHeaders=host&actor_id=48727989&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Den_vectors_web_lg-2.1.0.tar.gz&response-content-type=application%2Foctet-stream) as follows:\r\n```bash\r\npip install -r requirements.txt\r\nwget https://github.com/explosion/spacy-models/releases/download/en_vectors_web_lg-2.1.0/en_vectors_web_lg-2.1.0.tar.gz -O en_vectors_web_lg-2.1.0.tar.gz\r\npip install en_vectors_web_lg-2.\n... (truncated)"
    }
  }
]