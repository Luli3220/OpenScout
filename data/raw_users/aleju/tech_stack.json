[
  {
    "name": "aleju/imgaug",
    "stars": 14714,
    "description": "Image augmentation for machine learning experiments.",
    "languages_breakdown": {
      "Python": 5843500,
      "Shell": 609
    },
    "files": {
      "requirements.txt": "six\n# The test for imgaug.augmenters.blend.blend_alpha() fails for numpy<1.15.\n# All other tests seemed to work as of 2019/01/04.\nnumpy>=1.15\nscipy\nPillow\nmatplotlib\nscikit-image>=0.14.2\nopencv-python-headless\n# imageio versions past 2.6.1 do not support <3.5 anymore\nimageio<=2.6.1; python_version<'3.5'\nimageio; python_version>='3.5'\nShapely\n# numba is not available in <=3.5\nnumba; python_version>='3.6'\n",
      "README.md": "# imgaug\n\nThis python library helps you with augmenting images for your machine learning projects.\nIt converts a set of input images into a new, much larger set of slightly altered images.\n\n[![Build Status](https://travis-ci.org/aleju/imgaug.svg?branch=master)](https://travis-ci.org/aleju/imgaug)\n[![codecov](https://codecov.io/gh/aleju/imgaug/branch/master/graph/badge.svg)](https://codecov.io/gh/aleju/imgaug)\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/1370ce38e99e40af842d47a8dd721444)](https://www.codacy.com/app/aleju/imgaug?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=aleju/imgaug&amp;utm_campaign=Badge_Grade)\n\n<table>\n\n<tr>\n<th>&nbsp;</th>\n<th>Image</th>\n<th>Heatmaps</th>\n<th>Seg. Maps</th>\n<th>Keypoints</th>\n<th>Bounding Boxes,<br>Polygons</th>\n</tr>\n\n<!-- Line 1: Original Input -->\n<tr>\n<td><em>Original Input</em></td>\n<td><img src=\"https://raw.githubusercontent.com/aleju/imgaug-doc/master/readme_images/small_overview/noop_image.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"input images\"></td>\n<td><img src=\"https://raw.githubusercontent.com/aleju/imgaug-doc/master/readme_images/small_overview/noop_heatmap.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"input heatmaps\"></td>\n<td><img src=\"https://raw.githubusercontent.com/aleju/imgaug-doc/master/readme_images/small_overview/noop_segmap.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"input segmentation maps\"></td>\n<td><img src=\"https://raw.githubusercontent.com/aleju/imgaug-doc/master/readme_images/small_overview/noop_kps.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"input keypoints\"></td>\n<td><img src=\"https://raw.githubusercontent.com/aleju/imgaug-doc/master/readme_images/small_overview/noop_bbs.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"input bounding boxes\"></td>\n</tr>\n\n<!-- Line 2: Gauss. Noise + Contrast + Sharpen -->\n<tr>\n<td>Gauss. Noise<br>+&nbsp;Contrast<br>+&nbsp;Sharpen</td>\n<td><img src=\"https://raw.githubusercontent.com/aleju/imgaug-doc/master/readme_images/small_overview/non_geometric_image.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"non geometric augmentations, applied to images\"></td>\n<td><img src=\"https://raw.githubusercontent.com/aleju/imgaug-doc/master/readme_images/small_overview/non_geometric_heatmap.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"non geometric augmentations, applied to heatmaps\"></td>\n<td><img src=\"https://raw.githubusercontent.com/aleju/imgaug-doc/master/readme_images/small_overview/non_geometric_segmap.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"non geometric augmentations, applied to segmentation maps\"></td>\n<td><img src=\"https://raw.githubusercontent.com/aleju/imgaug-doc/master/readme_images/small_overview/non_geometric_kps.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"non geometric augmentations, applied to keypoints\"></td>\n<td><img src=\"https://raw.githubusercontent.com/aleju/imgaug-doc/master/readme_images/small_overview/non_geometric_bbs.jpg?raw=true\" height=\"83\" width=\"124\" alt=\"non geometric augmentations, applied to bounding boxes\"></td\n... (truncated)"
    }
  },
  {
    "name": "aleju/papers",
    "stars": 2503,
    "description": "Summaries of machine learning papers",
    "languages_breakdown": {},
    "files": {
      "README.md": "# About\n\nThis repository contains short summaries of some machine learning papers.\n\n## Added 2018/10/01:\n  * ****`UNSUPERVISED LEARNING`**** ****`ECCV 2018`**** [Deep Clustering for Unsupervised Learning of Visual Features](neural-nets/Deep_Clustering_for_Unsupervised_Learning_of_Visual_Features.md)\n  * ****`OBJECT DETECTION`**** ****`POINT CLOUD`**** ****`SELF-DRIVING CARS`**** ****`ECCV 2018`**** [Deep Continuous Fusion for Multi-Sensor 3D Object Detection](neural-nets/Deep_Continuous_Fusion_for_Multi-Sensor_3D_Object_Detection.md)\n  * ****`AUDIO`**** ****`SOUND SOURCE LOCALIZATION`**** ****`ACTION RECOGNITION`**** ****`SOUND SOURCE SEPARATION`**** ****`SELF-SUPERVISED`**** ****`ECCV 2018`**** [Audio-Visual Scene Analysis with Self-Supervised Multisensory Features](neural-nets/Audio-Visual_Scene_Analysis_with_Self-Supervised_Multisensory_Features.md)\n  * ****`UNCERTAINTY`**** ****`ECCV 2018`**** [Towards Realistic Predictors](neural-nets/Towards_Realistic_Predictors.md)\n  * ****`OBJECT DETECTION`**** ****`ECCV 2018`**** [Acquisition of Localization Confidence for Accurate Object Detection](neural-nets/Acquisition_of_Localization_Confidence_for_Accurate_OD.md)\n  * ****`OBJECT DETECTION`**** ****`ECCV 2018`**** [CornerNet: Detecting Objects as Paired Keypoints](neural-nets/CornerNet.md)\n  * ****`NORMALIZATION`**** ****`ECCV 2018`**** [Group Normalization](neural-nets/Group_Normalization.md)\n  * ****`ARCHITECTURES`**** ****`ATTENTION`**** ****`ECCV 2018`**** [Convolutional Networks with Adaptive Inference Graphs](neural-nets/Convolutional_Networks_with_Adaptive_Inference_Graphs.md)\n\n## Added 2018/03/08:\n  * ****`ARCHITECTURES`**** ****`ATTENTION`**** [Spatial Transformer Networks](neural-nets/STN.md) (thanks, [alexobednikov](https://github.com/alexobednikov))\n\n## Added 2018/03/06:\n  * ****`LOSS FUNCTIONS`**** ****`RECOGNITION`**** [Working hard to know your neighborâ€™s margins: Local descriptor learning loss](neural-nets/HardNet.md) (thanks, [alexobednikov](https://github.com/alexobednikov))\n\n## Added 2017/12/13:\n  * ****`FACE RECOGNITION`**** ****`FACES`**** [Neural Aggregation Network for Video Face Recognition](neural-nets/NAN_for_Video_Face_Recognition.md) (thanks, [alexobednikov](https://github.com/alexobednikov))\n\n## Added 2017/12/03:\n  * [Critical Learning Periods in Deep Neural Networks](neural-nets/Critical_Learning_Periods_in_Deep_Neural_Networks.md)\n  * ****`GAN`**** ****`SELF-DRIVING CARS`**** [High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs](neural-nets/High_Resolution_Image_Synthesis_with_Conditional_GANs.md)\n  * ****`SELF-DRIVING CARS`**** [Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art](mixed/Computer_Vision_for_Autonomous_Vehicles_Overview.md)\n\n## Added 2017/10/28:\n  * ****`GAN`**** [Progressive Growing of GANs for Improved Quality, Stability, and Variation](neural-nets/Progressive_Growing_of_GANs.md)\n\n## Added 2017/10/24:\n  * ****`SELF-DRIVING CARS`**** [Systematic \n... (truncated)"
    }
  },
  {
    "name": "aleju/mario-ai",
    "stars": 697,
    "description": "Playing Mario with Deep Reinforcement Learning",
    "languages_breakdown": {
      "Lua": 146655
    },
    "files": {
      "README.md": "# About\n\nThis project contains code to train a model that automatically plays the first level of Super Mario World using only raw pixels as the input (no hand-engineered features).\nThe used technique is deep Q-learning, as described in the [Atari paper](http://arxiv.org/abs/1312.5602) ([Summary](https://github.com/aleju/papers/blob/master/neural-nets/Playing_Atari_with_Deep_Reinforcement_Learning.md)), combined with a [Spatial Transformer](https://arxiv.org/abs/1506.02025).\n\n# Video\n\n[![Model playing SMW](images/youtube.png?raw=true)](https://www.youtube.com/watch?v=L4KBBAwF_bE)\n\n# Methodology\n\n## Basics, replay memory\n\nThe training method is deep Q-learning with a replay memory, i.e. the model observes sequences of screens,\nsaves them into its memory and later trains on them, where \"training\" means that it learns to accurately predict the expected action reward values\n(\"action\" means \"press button X\") based on the collected memories.\nThe replay memory has by default a size of 250k entries.\nWhen it starts to get full, new entries replace older ones.\nFor the training batches, examples are chosen randomly (uniform distribution) and rewards of memories are reestimated based on what the network has learned so far.\n\n## Inputs, outputs, actions\n\nEach example's input has the following structure:\n* The last T actions, each as two-hot-vectors. (Two, because the model can choose two buttons: One arrow-button and one of A/B/X/Y.)\n* The last T screenshots, each downscaled to size 32x32 (grayscale, slightly cropped).\n* The last screenshot, at size 64x64 (grayscale, slightly cropped).\n\nT is currently set to 4 (note that this includes the last state of the sequence). Screens are captured at every 5th frame.\nEach example's output are the action reward values of the chosen action (received direct reward + discounted Q-value of the next state).\nThe model can choose two actions per state: One arrow button (up, down, right, left) and one of the other control buttons (A, B, X, Y).\nThis is different from the Atari-model, in which the agent could only pick one button at a time.\n(Without this change, the agent could theoretically not make many jumps, which force you to keep the A button pressed and move to the right.)\nAs the reward function is constructed in such a way that it is almost never 0, exactly two of each example's output values are expected to be non-zero.\n\n## Reward function\n\nThe agent gets the following rewards:\n* X-Difference reward: `+0.5` if the agent moved to the right, `+1.0` if it moved *fast* to the right (8 pixels or more compared to the last game state), `-1.0` if it moved to the left and `-1.5` if it moved *fast* to the left (-8 pixels or more).\n* Level finished: `+2.0` while the level-finished-animation is playing.\n* Death: `-3.0` while the death animation is playing.\n\nThe `gamma` (discount for expected/indirect rewards) is set to `0.9`.\n\nTraining the model only on score increases (like in the Atari paper) would most likely not work, because enem\n... (truncated)"
    }
  }
]