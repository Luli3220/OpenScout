[
  {
    "name": "steveloughran/winutils",
    "stars": 2637,
    "description": "Windows binaries for Hadoop versions (built from the git commit ID used for the ASF relase)",
    "languages_breakdown": {
      "Shell": 266190,
      "Batchfile": 237705,
      "PowerShell": 2676
    },
    "files": {
      "README.md": "# winutils\nWindows binaries for Hadoop versions \n\nThese are built directly from the same git commit used to create the official ASF releases; they are checked out\nand built on a windows VM which is dedicated purely to testing Hadoop/YARN apps on Windows. It is not a day-to-day\nused system so is isolated from driveby/email security attacks.\n\n\n## Status November 10, 2022: Bare Nakes Local FS\n\n(Garret Wilson)[https://github.com/garretwilson] has implemented a filesystem which can be used as a replacement for the classic FS, without the need for winutils\n\n* (GlobalMentor Hadoop Bare Naked Local FileSystem)[https://github.com/globalmentor/hadoop-bare-naked-local-fs]\n\nIf this works for you, no need for winutils at all!\n\n## Status: Go to cdarlint/winutils for current artifacts\n\nI've been too busy with things to work on this for a long time, so I'm grateful for cdarlint to take up this work:\n[cdarlint/winutils](https://github.com/cdarlint/winutils).\n\nIf you want more current binaries, please go there.\n\nDo note that given some effort it should be possible to avoid the Hadoop `file://` classes (Local and RawLocal) to need the hadoop native\nlibs except in the special case that you are doing file permissions work. If someone wants to do some effort into cutting the need for\nthese libs on Windows systems just to run Spark & similar locally, file a JIRA [on Apache](https://issues.apache.org/jira/secure/Dashboard.jspa), then a PR against [apache/hadoop](https://github.com/apache/hadoop). Thanks\n\n## Security: can you trust this release?\n\n1. I am the Hadoop committer \" stevel\": I have nothing to gain by creating malicious versions of these binaries. If I wanted to run anything on your systems, I'd be able to add the code into Hadoop itself.\n1. I'm signing the releases.\n1. My keys are published on the ASF committer keylist [under my username](https://people.apache.org/keys/committer/stevel).\n1. The latest GPG key (E7E4 26DF 6228 1B63 D679  6A81 950C C3E0 32B7 9CA2) actually lives on a yubikey for physical security; the signing takes place there.\n1. The same pubikey key is used for 2FA to github, for uploading artifacts and making the release.\n\nSomeone malicious would need physical access to my office to sign artifacts under my name. If they could do that, they could commit malicious code into Hadoop itself, even signing those commits with the same GPG key. Though they'd need the pin number to unlock the key, which I have to type in whenever the laptop wakes up and I want to sign something. That'd take getting something malicious onto my machine, or sniffing the bluetooth packets from the keyboard to laptop. Were someone to get physical access to my machine, they could probably install a malicous version of `git`, one which modified code before the checkin. I don't actually my patches to verify that there's been no tampering, but we do tend to keep an eye on what our peers put in.\n\nThe other tactic would have been for a malicious yubikey to end up being delivered b\n... (truncated)"
    }
  },
  {
    "name": "steveloughran/kerberos_and_hadoop",
    "stars": 280,
    "description": "Kerberos and Hadoop: The Madness beyond the Gate",
    "languages_breakdown": {},
    "files": {
      "README.md": "\n# Hadoop and Kerberos: The Madness beyond the Gate\n\n\n> The most merciful thing in the world, I think, is the inability of the human mind to correlate all its contents.\n> We live on a placid island of ignorance in the midst of black seas of infinity, and it was not meant that we should voyage far.\n> The sciences, each straining in its own direction, have hitherto harmed us little;\n> but some day the piecing together of dissociated knowledge will open up such terrifying vistas of reality,\n> and of our frightful position therein, that we shall either go mad from the revelation\n> or flee from the light into the peace and safety of a new dark age.\n\n> *[The Call of Cthulhu](https://en.wikisource.org/wiki/The_Call_of_Cthulhu), HP Lovecraft, 1926.*\n\n\nThis manuscript discusses low-level issues related to Apache&trade; Hadoop&reg; and Kerberos\n\n## Disclaimer\n\nJust as the infamous [Necronomicon](http://www.amazon.com/gp/product/0380751925) is a collection\nof notes scrawled in blood as a warning to others, this book is\n\n1. Incomplete.\n1. Based on experience and superstition, rather than understanding and insight.\n1. Contains information that will drive the reader insane.\n\nReading this book implies recognition of these facts and that the reader, their estate and\ntheir heirs accept all risk and liability. The author is not responsible if anything happens\nto their Apache Hadoop cluster, including all the data stored in HDFS disappearing into an unknown dimension,\nor the YARN scheduler starting to summon pre-human deities.\n\n**You have been warned**\n\n\n## Implementation notes.\n\n1. This is a work in progress book designed to built using the [gitbook tool chain](https://github.com/GitbookIO/gitbook).\n\n1. It is hosted on [github](https://github.com/steveloughran/kerberos_and_hadoop).\nPull requests are welcome.\n\n1. All the content is Apache licensed.\n\n1. This is not a formal support channel for Hadoop + Kerberos problems. If you have a support\ncontract with [Cloudera](http://cloudera.com/) then issues related to Kerberos may \neventually reach the author. Otherwise: try \n\n      - [Cloudera Community](https://community.cloudera.com/)\n      - The users mailing list of Apache Hadoop, the application and you are using on top of it.\n      - [Stack Overflow](http://stackoverflow.com/search?q=hadoop+kerberos).\n1. The author is very much *not* a Kerberos expert, so (a) he can be wrong and (b) asking hard questions about it will generally get a \"I have no idea whatsoever\" answer. \n"
    }
  },
  {
    "name": "steveloughran/cloudstore",
    "stars": 36,
    "description": "Hadoop utility jar for troubleshooting integration with cloud object stores",
    "languages_breakdown": {
      "Java": 692118
    },
    "files": {
      "pom.xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!--\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n\n  <groupId>com.cloudera.cloud</groupId>\n  <artifactId>cloudstore</artifactId>\n  <version>1.1</version>\n  <packaging>jar</packaging>\n\n  <name>cloudstore</name>\n  <url>https://github.com/steveloughran/cloudstore</url>\n\n\n  <description>\n    Cloud Store operations, targeting Hadoop 3.3.0+.\n\n    The code is all in the org.apache.hadoop package as we have a goal of\n    including it in hadoop at some point. It just so happens that\n    it turns out to be very useful to be able to agile releases, sometimes\n    even more than once a day, so that we can track down problems elsewhere.\n  </description>\n\n  <properties>\n    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    <file.encoding>UTF-8</file.encoding>\n    <javac.version>1.8</javac.version>\n    <enforced.java.version>${javac.version}</enforced.java.version>\n\n    <hadoop.version>3.4.2</hadoop.version>\n    <gcs-connector.version>hadoop3-2.2.4</gcs-connector.version>\n\n    <test.build.dir>${project.build.directory}/test-dir</test.build.dir>\n    <test.build.data>${test.build.dir}</test.build.data>\n    <assertj.version>3.12.2</assertj.version>\n    <buildhelper.version>3.3.0</buildhelper.version>\n\n  </properties>\n\n  <dependencies>\n    <dependency>\n      <groupId>junit</groupId>\n      <artifactId>junit</artifactId>\n      <version>4.13.2</version>\n      <scope>provided</scope>\n    </dependency>\n    <dependency>\n      <groupId>org.assertj</groupId>\n      <artifactId>assertj-core</artifactId>\n      <version>${assertj.version}</version>\n      <scope>test</scope>\n    </dependency>\n    <!--\n\n        <dependency>\n          <groupId>org.slf4j</groupId>\n          <artifactId>slf4j-log4j12</artifactId>\n          <version>1.7.32</version>\n          <scope>provided</scope>\n        </dependency>\n    -->\n\n\n    <!-- https://mvnrepository.com/artifact/ch.qos.logback/logback-classic -->\n    <!--\n        <dependency>\n          <groupId>ch.qos.logback</groupId>\n          <artifactId>logback-classic</artifactId>\n          <version>1.4.5</version>\n          <scope>provided</scope>\n        </dependency>\n    -->\n\n    <dependency>\n      <groupId>org.apache.hadoop</gr\n... (truncated)",
      "README.md": "<!---\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n\n# Cloudstore\n\nCloudstore is a diagnostics library for troubleshooting problems\ninteracting with cloud storage through Apache Hadoop\n\n*License*: Apache ASF 2.0\n\nAll the implementation classes are under the `org.apache.hadoop.fs` package\ntree but it is not part of the apache hadoop artifacts.\n\n1. Faster release cycle, so the diagnostics can evolve to track features going\n   into Hadoop-trunk.\n2. Fewer test requirements. This is naughty, but as much of this code is written\n   in a hurry to diagnose problems on remote sites, problems which are hard to test,\n   it is under-tested.\n3. Ability to compile against older versions. We've currently switched to Hadoop 3.3+\n   due to the need to make API calls and operations not in older versions.\n\n\n## Features\n\n### Primarily: diagnostics\n\nWhy? \n\n1. Sometimes things fail, and the first problem is classpath;\n2. The second, invariably some client-side config. \n3. Then there's networking and permissions...\n4. The Hadoop FS connectors all assume a well configured system, and don't\ndo much in terms of meaningful diagnostics.\n5. This is compounded by the fact that we dare not log secret credentials.\n6. And in support calls, it's all to easy to get those secrets, even\nthough its a major security breach to get them.\n\n### Secondary: higher performance cloud IO\n\nThe main hadoop `hadoop fs` commands are written assuming a filesystem, where:\n\n* Recursive treewalks are the way to traverse the store.\n* The code was written for Hadoop 1.0 and uses the filesystem APIs of that era.\n* The commands are often used in shell scripts and workflows, including parsing\nthe output: we do not dare change the behaviour or output for this reason.\n* And the shell removes stack traces on failures, making it of \"limited value\"\nwhen things don't work. And object stores are fairly fussy to get working, \nprimarily due to authentication, classpath and network settings\n\n## See also\n\n* [Security](./SECURITY.md)\n* [Building](./BUILDING.md)\n\n\n# Commands\n\n## Common arguments\n\nThere are a set of arguments common to all commands\n```\n-D <key=value>          Define a property\n-sysprops <file>        Java system properties to set\n-tokenfile <file>       Hadoop token file to load\n-verbose                Verbose output\n-debug                  Extra debug logs (JVM and Log4j overrides)\n-logoverrides <file>    A newline separated list of packages and classes for Log4j overrides\n-xmlfile <fi\n... (truncated)"
    }
  }
]