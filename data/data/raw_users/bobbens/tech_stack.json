[
  {
    "name": "bobbens/sketch_simplification",
    "stars": 750,
    "description": "Models and code related to sketch simplification of rough sketches.",
    "languages_breakdown": {
      "Lua": 30428,
      "Shell": 1944,
      "Python": 1129
    },
    "files": {
      "README.md": "# [Sketch Simplification](https://esslab.jp/~ess/research/sketch/)\n\n![Example result](/example_fig01_eisaku.png?raw=true \"Example result of the provided model.\")\nExample result of a sketch simplification. Image copyrighted by Eisaku Kubonouchi ([@EISAKUSAKU](https://twitter.com/eisakusaku)) and only non-commercial research usage is allowed.\n\n## Overview\n\nThis code provides pre-trained models used in the research papers:\n\n```\n   \"Learning to Simplify: Fully Convolutional Networks for Rough Sketch Cleanup\"\n   Edgar Simo-Serra*, Satoshi Iizuka*, Kazuma Sasaki, Hiroshi Ishikawa (* equal contribution)\n   ACM Transactions on Graphics (SIGGRAPH), 2016\n```\n\nand\n\n```\n   \"Mastering Sketching: Adversarial Augmentation for Structured Prediction\"\n   Edgar Simo-Serra*, Satoshi Iizuka*, Hiroshi Ishikawa (* equal contribution)\n   ACM Transactions on Graphics (TOG), 2018\n```\n\nSee our [project page](https://esslab.jp/~ess/research/sketch_master/) for more detailed information.\n\n## Dependencies\n\n- [PyTorch](http://pytorch.org/) (version 0.4.1)\n  [torchvision](http://pytorch.org/docs/master/torchvision/)\n- [pillow](http://pillow.readthedocs.io/en/latest/index.html)\n\nAll packages should be part of a standard PyTorch install. For information on how to install PyTorch please refer to the [torch website](http://pytorch.org/).\n\n## Usage\n\nBefore the first usage, the models have to be downloaded with:\n\n```\nbash download_models.sh\n```\n\nNext test the models with:\n\n```\npython simplify.py\n```\n\nYou should see a file called `out.png` created with the output of the model.\n\nApplication options can be seen with:\n\n```\npython simplify.py --help\n```\n\n## Pencil Drawing Generation\n\nUsing the same interface it is possible to perform pencil drawing generation. In this case, the input should be a clean line drawing and not a rough sketch, and the line drawings can be generated by:\n\n```\npython simplify.py --img test_line.png --out out_rough.png --model model_pencil2.t7\n```\n\nThis will generate a rough version of `test_line.png` as `out_rough.png`. By changing the model it is possible to change the type of rough sketch being generated.\n\n## Models\n\n- `model_mse.t7`: Model trained using only MSE loss (SIGGRAPH 2016 model).\n- `model_gan.t7`: Model trained with MSE and GAN loss using both supervised and unsupervised training data (TOG 2018 model).\n- `model_pencil1.t7`: Model for pencil drawing generation based on artist 1 (dirty and faded pencil lines).\n- `model_pencil2.t7`: Model for pencil drawing generation based on artist 2 (clearer overlaid pencil lines).\n\n## Reproducing Paper Figures\n\nFor replicability we include code to replicate the figures in the paper. After downloading the models you can run it with:\n\n```\n./figs.sh\n```\n\nThis will convert the input images in `figs/` and save the output in `out/`. We note that there are small differences with the results in the paper due to hardware differences and small differences in the torch/pytorch implementations. Furthermore, results are shown wit\n... (truncated)"
    }
  },
  {
    "name": "bobbens/clothes_parsing",
    "stars": 118,
    "description": "Code for the paper 'A High Performance CRF Model for Clothes Parsing'.",
    "languages_breakdown": {
      "C++": 2971753,
      "MATLAB": 629234,
      "C": 117589,
      "Makefile": 25320,
      "Python": 14313,
      "Gherkin": 3136,
      "Lex": 2585,
      "Shell": 2041,
      "M": 396
    },
    "files": {
      "README.md": "Clothes Parsing\n===============\n\nOverview\n--------\n\nThis code provides an implementation of the research paper:\n\n```\n  A High Performance CRF Model for Clothes Parsing\n  Edgar Simo-Serra, Sanja Fidler, Francesc Moreno-Noguer, and Raquel Urtasun\n  Asian Conference on Computer Vision (ACCV), 2014\n```\n\nThe code here allows training and testing of a model that got state-of-the-art\nresults on the\n[Fashionista](http://vision.is.tohoku.ac.jp/~kyamagu/ja/research/clothing_parsing/)\ndataset at the time of publication.\n\n\nLicense\n-------\n\n```\n  Copyright (C) <2014> <Edgar Simo-Serra, Sanja Fidler, Francesc Moreno-Noguer, Raquel Urtasun>\n\n  This work is licensed under the Creative Commons\n  Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy\n  of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/ or\n  send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n  Edgar Simo-Serra, Institut de Robotica i Informatica Industrial (CSIC/UPC), December 2014.\n  esimo@iri.upc.edu, http://www-iri.upc.es/people/esimo/\n```\n\n\nInstallation\n------------\n\nIn order to get started first checkout out the source code and then extract the\nfeatures:\n\n```\n# Check out the git and cd into it as working directory\ngit clone https://github.com/bobbens/clothes_parsing.git\ncd clothes_parsing\n# Get and unpack the necessary features\nwget http://hi.cs.waseda.ac.jp/~esimo//data/poseseg.tar.bz2\ntar xvjf poseseg.tar.bz2 \n```\n\nThe\n[dSP](http://www.alexander-schwing.de/projectsGeneralStructuredPredictionLatentVariables.php)\ndependency must also be compiled. This can be done by:\n\n```\ncd lib/dSP_5.1\nmake # First edit the Makefile if necessary\n```\n\nUsage\n-----\n\nYou can reproduce results simply by running from Matlab:\n\n```\nsm = segmodel( 'PROFILE', '0.16', 'use_real_pose', false ); % Load the model, parameters can be set here\nsm = sm.train_misc_unaries(); % Trains some misc stuff\nsm = sm.train_MRF(); % Actually sets up and trains the CRF\nR = sm.test_MRF_segmentation() % Performs testing and outputs results\n```\n\nThis should generate an output like:\n\n```\n BUILDING MRF OUTPUT 29 CLASSES (REAL POSE=0)...\n UNARIES:\n    bgbias\n    logreg:       29\n    cpmc_logreg:  29\n    cpmc\n    shapelets\n HIGHER ORDER\n    similarity\n    limbs\n Initializing Image 011 / 350...   0.4 seconds!   \n\n ...\n\n Tested MRF in 319.0 seconds\n 350 / 350... \n\n R = \n\n     confusion: [29x29 double]\n     order: [29x1 double]\n     acc: 0.8432\n     pre: [29x1 double]\n     rec: [29x1 double]\n     f1: [29x1 double]\n     voc: [29x1 double]\n     avr_pre: 0.3007\n     avr_rec: 0.3292\n     avr_f1: 0.3039\n     avr_voc: 0.2013\n```\n\nPlease note that due to stochastic components and differences between software\nversions, the numbers will not be exactly the same as the paper. For the paper\nall results were obtained on a linux machine running Ubuntu 12.04 with Matlab\nR2012a (7.14.0.739) 64-bit (glnxa64).\n\nYou can furthermore visualize the output of the model with:\n\n```\nsm.test_MRF_visuali\n... (truncated)"
    }
  },
  {
    "name": "bobbens/cvpr2016_stylenet",
    "stars": 68,
    "description": "Code for our CVPR 2016 paper on Fashion styles in 128 floats.",
    "languages_breakdown": {
      "Lua": 1529
    },
    "files": {
      "README.md": "# [Fashion Style in 128 Floats](http://hi.cs.waseda.ac.jp/~esimo/research/stylenet/)\n\n[Edgar Simo-Serra](http://hi.cs.waseda.ac.jp/~esimo/), [Hiroshi Ishikawa](http://www.f.waseda.jp/hfs/indexE.html)\n\n## Overview\n\nThis code provides an implementation of the research paper:\n\n```\n  \"Fashion Style in 128 Floats: Joint Ranking and Classification using Weak Data for Feature Extraction\"\n  Edgar Simo-Serra and Hiroshi Ishikawa\n  Conference in Computer Vision and Pattern Recognition (CVPR), 2016\n```\n\nSee our [project page](http://hi.cs.waseda.ac.jp/~esimo/research/stylenet/) for more detailed information.\n\n## License\n\n```\n  Copyright (C) <2016> <Edgar Simo-Serra>\n\n  This work is licensed under the Creative Commons\n  Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy\n  of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/ or\n  send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n  Edgar Simo-Serra, Waseda University\n  esimo@aoni.waseda.jp, http://hi.cs.waseda.ac.jp/~esimo/  \n```\n\n\n## Dependencies\n\n- [Torch7](http://torch.ch/docs/getting-started.html)\n- [nn](https://github.com/torch/nn)\n- [image](https://github.com/torch/image)\n\nAll packages should be part of a standard Torch7 install. For information on how to install Torch7 please see the [official torch documentation](http://torch.ch/docs/getting-started.html) on the subject.\n\n## Usage\n\nTest the model with\n\n```\nth test.lua\n```\n\nYou should see a 7x7 matrix displayed on screen which are the descriptor distance values between the seven example images provided in this repository.\n\n### Notes\n\n- Model provided is the best performing model corresponding to \"Ours Joint\" in the paper.\n- This was developed on a linux machine running Ubuntu 14.04 during late 2015.\n- The provided code does not use GPU accelerated (trivial to change).\n- Provided model and sample code is under a non-commercial creative commons license.\n\n## Dataset\n\nThe model was trained on a \"clean\" subset of the [Fashion144k\ndataset](http://hi.cs.waseda.ac.jp/~esimo/ja/research/fashionability/). The\ndataset used will be released shortly.\n\n## Citing\n\nIf you use this code please cite:\n\n```\n@InProceedings{SimoSerraCVPR2016,\n   author    = {Edgar Simo-Serra and Hiroshi Ishikawa},\n   title     = {{Fashion Style in 128 Floats: Joint Ranking and Classification using Weak Data for Feature Extraction}},\n   booktitle = \"Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)\",\n   year      = 2016,\n}\n```\n\n\n\n\n"
    }
  }
]