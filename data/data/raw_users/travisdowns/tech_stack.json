[
  {
    "name": "travisdowns/uarch-bench",
    "stars": 754,
    "description": "A benchmark for low-level CPU micro-architectural features",
    "languages_breakdown": {
      "C++": 4106712,
      "Assembly": 88833,
      "C": 18523,
      "Shell": 9693,
      "Makefile": 7774,
      "Python": 199
    },
    "files": {
      "README.md": "[![Build Status](https://travis-ci.org/travisdowns/uarch-bench.svg?branch=master)](https://travis-ci.org/travisdowns/uarch-bench)\n\n# uarch-bench\n\nA collection of low level, fine-grained benchmarks intended to investigate micro-architectural details of a target CPU, or to precisely benchmark small functions in a repeatable manner.\n\n## Disclaimer\n\n**This project is very much a work-in-progress, and is currently in a very early state with limited documentation and testing. Pull requests and issues welcome.**\n\n## Purpose\n\nThe uarch-bench project is a collection of micro-benchmarks that try to stress certain micro-architectural features of modern CPUs and a framework for writing such benchmarks. Using [libpfc](https://github.com/obilaniu/libpfc) you can accurately track the value of Intel performance counters across the benchmarked region - often with precision of a single cycle.\n\nAt the moment it supports only x86, using mostly assembly and a few C++ benchmarks. In the future, I'd like to have more C or C++ benchmarks, allowing coverage (in principle) of more platforms (non-x86 assembly level benchmarks are also welcome). Of course, for any non-asm benchmark, it is possible that the compiler makes a transformation that invalidates the intent of the benchmark. You could detect this as a large difference between the C/C++ and assembly scores.\n\nOf course, these have all the pitfalls of any microbenchmark and are not really intended to be a simple measure of the overall\nperformance of any CPU architecture. Rather they are mostly useful to:\n\n1. Suss out changes between architectures. Often there are changes to particular micro-architectural feature that can be exposed\nvia benchmarks of specific features. For example, you might be able to understand something about the behavior of the store buffer based on tests that exercise store-to-load forwarding.\n2. Understand low-level performance of various approaches to guide implementation of highly-tuned algorithms. For the vast\nmajority of typical development tasks, the very low level information provided by these benches is essentially useless in\nproviding any guidance about performance. For some very specific tasks, such as highly-tuned C or C++ methods or hand-written\nassembly, it might be useful to characterize the performance of, for example, the relative costs of aligned and unaligned accesses, or whatever.\n3. Satisfy curiosity for those who care about this stuff and to collect the results from various architectures.\n4. Provide a simple, standard way to quickly do one-off tests of some small assembly or C/C++ level idioms. Often the\ntest itself is a few lines of code, but the cost is in all the infrastructure: implementing the timing code, converting measurements to cycles, removing outliers, running the tests for various parameters, reporting the results, whatever. This\nproject aims to implement that infrastructure and make it easy to add your own tests (not complete!).\n\n\n## Platform support\n\nCurrently on\n... (truncated)"
    }
  },
  {
    "name": "travisdowns/avx-turbo",
    "stars": 229,
    "description": "Test the non-AVX, AVX2 and AVX-512 speeds across various active core counts",
    "languages_breakdown": {
      "C++": 809658,
      "C": 47676,
      "Assembly": 13928,
      "Makefile": 1612,
      "Shell": 558
    },
    "files": {
      "README.md": "# avx-turbo\n\nTest the non-AVX, AVX2 and AVX-512 speeds for various types of CPU intensive loops with varying scalar and SIMD instructions, across different active core counts.\n\nCurrently it is **Linux only** (it does run on WSL and WSL2 on Windows), but the basic testing mechanism could be ported to OSX and Windows as well (help welcome). \n\n# CI Status\n\n**Build:** [![Master Branch](https://github.com/travisdowns/avx-turbo/workflows/build/badge.svg)](https://github.com/travisdowns/avx-turbo/actions?query=workflow%3Abuild+branch%3Amaster)\n\n\n# build\n\n    make\n    \n# msr kernel module\n\nYou should load the `msr` kernel module if it is not already loaded. This is as simple as:\n\n    modprobe msr\n\nOr as complex as (if you want nice messages about what happened):\n\n    lsmod | grep -q msr && echo \"MSR already loaded\" || { echo \"Loading MSR module\"; sudo modprobe msr ; }\n\n# run\n\nYou get the most info running as root (since we can read various MSRs to calculate the frequency directly):\n\n    sudo ./avx-turbo\n\nYou can also run it without root, but you only get the \"Mops\" reading (but this can be read directly as frequency\nfor the 1-latency tests). \n\n## spec-based tests\n\nThe default behavior for ./avx-turbo is to run tests with various thread counts, but with the same test on each thread. For example, the `avx256_fma` test means that the same FMA-using test code will be run on _each_ test thread.\n\nAn alternate approach is availe with so-called _spec-based_ tests. This lets you specificy exactly what each thread in a test will run. The general form of a specification is: `test1/thead-count1[,test2/thread-count2[,...]]`. For example,\nif you run `sudo ./avx-turbo --spec avx256_fma/1,scalar_iadd/3` you'll get one copy of `avx256_fma` and three copies of `scalar_iadd` running in parallel.\n\nThis mode is useful to testing that happens when not all cores are doing the same thing.\n\n# help\n\nTry:\n\n    ./avx-turbo --help\n    \nfor a summary of some options something like this:\n\n```\n  ./avx-turbo {OPTIONS}\n\n    avx-turbo: Determine AVX2 and AVX-512 downclocking behavior\n\n  OPTIONS:\n\n      -h, --help                        Display this help menu\n      --force-tsc-calibrate             Force manual TSC calibration loop, even\n                                        if cpuid TSC Hz is available\n      --no-pin                          Don't try to pin threads to CPU - gives\n                                        worse results but works around affinity\n                                        issues on TravisCI\n      --verbose                         Output more info\n      --no-barrier                      Don't sync up threads before each test\n                                        (no real purpose)\n      --list                            List the available tests and their\n                                        descriptions\n      --allow-hyperthreads              By default we try to filter down the\n                                        available cpus to include only physical\n... (truncated)"
    }
  },
  {
    "name": "travisdowns/robsize",
    "stars": 158,
    "description": "ROB size testing utility",
    "languages_breakdown": {
      "C++": 28233,
      "Python": 9665,
      "Shell": 7485,
      "Makefile": 317
    },
    "files": {
      "README.md": "[![Build Status](https://travis-ci.org/travisdowns/robsize.svg?branch=master)](https://travis-ci.org/travisdowns/robsize)\n\nThis is an updated copy of the [ROB](https://en.wikipedia.org/wiki/Re-order_buffer) size testing tool described by [Henry Wong on his blog](http://blog.stuffedcow.net/2013/05/measuring-rob-capacity) and reproduced here with permission. It can be used to measure the size of not only the ROB, but various other microarchitectural buffer sizes, such as the load and store buffers, the GP, vector and mask phystical register file sizes, and so on. It includes several additional tests beyond those used in the original blog post.\n\nBuild it with `make` and run it like `./robsize ID` where `ID` is a test ID from 0 to 33 (run `./robsize --list` to show the available tests).\n\nThere are a few options that you can see with `./robsize --help`:\n\n```\nUsage: robsize [TEST_ID] [OPTIONS]\n\n\t--csv      \t  Output in csv format suitable for plotting\n\t--slow     \t  Run more iterations making the test slower but potentiallly more accurate\n\t--fast     \t  Run fewer iterations making the test faster but potentiallly less accurate\n\t--superfast\t  Run at ludicrous speed which is even less accurate than --fast\n\t--write-asm\t  Print the raw generated instructions to a file and quit\n\t--list     \t  List the available tests and their IDs\n\t--start=START Use START to specify the initial value of filler instruction count (Default = 16)\n\t--stop=STOP   Use STOP to specify the maximum value of filler instruction count (Default = 256)\n```\n\n## Supported Platforms\n\n### Operating Systems\n\nThis should run fine on any modern Linux, including [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/about).\n\n### Hardware\n\nSome tests require AVX, AVX2 or AVX-512, which should mostly be obvious from the description in `robsize --list`. There is no checking whether the current hardware supports the needed ISA, running those specific tests on hardware that doesn't support it will simply crash with an Illegal Instruction fault (pull requests welcome).\n\n## Interesting Tests\n\nThere are currently 27 different tests, so here I'll quickly note which are the most interesting for determining the main parameters of your system.\n\nA common theme is that many of the tests will probably detect the _same_ architectural feature (e.g., the ROB size), but do it in a slightly different way (e.g, with different type of nops, or different ALU operations, etc). One would expect them to give the same result, but if they give different results then they may reveal something interesting about the system. So the tests below aren't the _only_ ones that detect the particular feature!\n\nAll of the tests use the same basic structure: memory accesses that miss in the cache, separated by different types of operations (instructions). Only the operations vary, so when we say that a test \"uses single byte NOPs\" (for example), it is to be understood that the test consists as always of memory accesse\n... (truncated)"
    }
  }
]