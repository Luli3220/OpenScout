{
  "username": "xupefei",
  "agent_b_context": "\n=== PR #5291 in delta-io/delta (Stars: 8468) ===\n\n=== PR #5096 in delta-io/delta (Stars: 8468) ===\n\n=== PR #4158 in delta-io/delta (Stars: 8468) ===\n\n=== Fallback Source Audit: python/pyspark/sql/functions/builtin.py (Size: 880100 bytes) ===\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\"\"\"\nA collections of builtin functions\n\"\"\"\nimport inspect\nimport decimal\nimport sys\nimport functools\nimport warnings\nfrom typing import (\n    Any,\n    cast,\n    Callable,\n    Mapping,\n    Sequence,\n    Iterable,\n    overload,\n    Optional,\n    Tuple,\n    Type,\n    TYPE_CHECKING,\n    Union,\n    ValuesView,\n)\n\nfrom pyspark.errors import PySparkTypeError, PySparkValueError\nfrom pyspark.errors.utils import _with_origin\nfrom pyspark.sql.column import Column\nfrom pyspark.sql.dataframe import DataFrame as ParentDataFrame\nfrom pyspark.sql.types import (\n    ArrayType,\n    ByteType,\n    DataType,\n    StringType,\n    StructType,\n    NumericType,\n    _from_numpy_type,\n)\n\n# Keep UserDefinedFunction import for backwards compatible import; moved in SPARK-22409\nfrom pyspark.sql.udf import UserDefinedFunction, _create_py_udf  # noqa: F401\nfrom pyspark.sql.udtf import AnalyzeArgument, AnalyzeResult  # noqa: F401\nfrom pyspark.sql.udtf import OrderingColumn, PartitioningColumn, SelectedColumn  # noqa: F401\nfrom pyspark.sql.udtf import SkipRestOfInputTableException  # noqa: F401\nfrom pyspark.sql.udtf import UserDefinedTableFunction, _create_py_udtf\n\n# Keep pandas_udf and PandasUDFType import for backwards compatible import; moved in SPARK-28264\nfrom pyspark.sql.pandas.functions import pandas_udf, PandasUDFType  # noqa: F401\n\nfrom pyspark.sql.utils import (\n    to_str as _to_str,\n    try_remote_functions as _try_remote_functions,\n    get_active_spark_context as _get_active_spark_context,\n    enum_to_value as _enum_to_value,\n)\n\nif TYPE_CHECKING:\n    from pyspark import SparkContext\n    from pyspark.sql._typing import (\n        ColumnOrName,\n        DataTypeOrString,\n        UserDefinedFunctionLike,\n    )\n\n\n# Note to developers: all of PySpark functions here take string as column names whenever possible.\n# Namely, if columns are referred as arguments, they can always be both Column or string,\n# even though there might be few exceptions for legacy or inevitable reasons.\n# If you are fixing other language APIs together, also please note that Scala side is not the case\n# since it requires making every single overridden definition.\n\n\ndef _get_jvm_function(name: str, sc: \"SparkContext\") -> Callable:\n    \n... (file truncated)\n"
}